{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = \"../data/pivot_table.xlsx\"\n",
    "dfs = pd.read_excel(pivot_table, sheet_name=None)\n",
    "dfs_pivot_original = dfs[\"Сводная ПП+Потребности\"]\n",
    "dfs_potrebnosti_orginal = dfs[\"Потребности\"]\n",
    "dfs_pp_original = dfs[\"ПП\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Потребитель', 'Отдел', 'Ответственный', 'Установка', 'Материал ПП',\n",
       "       'Материал потребитель', 'Товар', 'Производитель', 'Сумма ПП',\n",
       "       'Сумма Потребности', 'Срок поставки', 'Вероятность следующей продажи',\n",
       "       'Конкуренты', 'Ситуация', 'Решение/следующее действие', 'Дата контроля',\n",
       "       'Примечание'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_pivot_original.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Потребитель', 'Материал', 'Сумма потребности', 'Срок поставки',\n",
       "       'Примечание', 'Отдел', 'Ответственный'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_potrebnosti_orginal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_col = [\"Потребитель\", \"Отдел\", 'Ответственный']\n",
    "col_compare = [\"Материал потребитель\", \"Материал\"] \n",
    "col4match_potreb_2 = [\"Потребитель\", \"Отдел\" ,\"Ответственный\",'Материал','Примечание']\n",
    "col4match_pivot_1= [\"Потребитель\", \"Отдел\" ,\"Ответственный\", \"Материал ПП\", 'Установка', 'Товар', 'Производитель']\n",
    "col4match_pivot_2 = [\"Потребитель\", \"Отдел\" ,\"Ответственный\",\"Материал потребитель\",'Примечание']\n",
    "col4match_pp = [\"Потребитель\", \"Отдел\" ,\"МП\", \"Материал, применение\", 'Установка', 'Товар', 'Производитель']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/wormsin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/wormsin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/wormsin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Загрузка стоп-слов и других ресурсов NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Функция для замены знаков препинания пробелами\n",
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):  # Проверяем, что text это строка\n",
    "        s_cleaned = re.sub(r'^\\s+|\\s+$', '', text) # пробелы в начале и в конце строки \n",
    "        return re.sub(r'[^\\w\\s]', ' ', s_cleaned)  # Заменяем все символы, кроме букв, цифр и пробелов, на пробелы\n",
    "    return text\n",
    "\n",
    "# Функция для приведения текста к нижнему регистру\n",
    "def to_lowercase(text):\n",
    "    if isinstance(text, str):  # Проверяем, что text это строка\n",
    "        return text.lower()\n",
    "    return text\n",
    "\n",
    "# Функция для удаления цифр\n",
    "def remove_numbers(text):\n",
    "    if isinstance(text, str):  # Проверяем, что text это строка\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Функция для удаления стоп-слов (предлогов, союзов и т.д.)\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  # Проверяем, что text это строка\n",
    "        stop_words = set(stopwords.words('russian'))  # Можно использовать 'english' для английского текста\n",
    "        words = nltk.word_tokenize(text)\n",
    "        return ' '.join([word for word in words if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Функция для стемминга\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def stem_text(text):\n",
    "    if isinstance(text, str):  # Проверяем, что text это строка\n",
    "        words = nltk.word_tokenize(text)\n",
    "        return ' '.join([stemmer.stem(word) for word in words])\n",
    "    return text\n",
    "\n",
    "def remove_nan_rows(df, column_name):\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Столбец {column_name} не найден в DataFrame\")\n",
    "    # Удалить строки с NaN в указанном столбце\n",
    "    df_cleaned = df.dropna(subset=[column_name])\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_column(df, column, oper):\n",
    "    if oper[5] == 1:\n",
    "        df = remove_nan_rows(df, column)\n",
    "    if oper[0] == 1:\n",
    "        df[column] = df[column].apply(remove_punctuation)\n",
    "    if oper[1] == 1:\n",
    "        df[column] = df[column].apply(to_lowercase)\n",
    "    if oper[2] == 1:\n",
    "        df[column] = df[column].apply(remove_numbers)\n",
    "    if oper[3] == 1:\n",
    "        df[column] = df[column].apply(stem_text)\n",
    "    if oper[4] == 1:\n",
    "        df[column] = df[column].apply(remove_stopwords)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'remove_punctuation',\n",
       " 1: 'to_lowercase',\n",
       " 2: 'remove_numbers',\n",
       " 3: 'stem_text',\n",
       " 4: 'remove_stopwords',\n",
       " 5: 'remove_nan_rows'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_strings = [\"remove_punctuation\", \"to_lowercase\", \"remove_numbers\", \"stem_text\", \"remove_stopwords\", \"remove_nan_rows\"]\n",
    "dict_from_list = {i: s for i, s in enumerate(list_of_strings)}\n",
    "dict_from_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_pivot(dfs):\n",
    "    dfs =clean_text_column(dfs, \"Материал ПП\", [1, 1, 0, 1, 1, 0])\n",
    "    dfs =clean_text_column(dfs, \"Товар\", [1, 1, 0, 0, 1, 0])\n",
    "    dfs =clean_text_column(dfs, \"Установка\", [1, 1, 0, 0, 0, 0])\n",
    "    dfs =clean_text_column(dfs, \"Производитель\", [1, 1, 0, 0, 0, 0])\n",
    "    \n",
    "    dfs = clean_text_column(dfs, \"Материал потребитель\", [1, 1, 0, 1, 1, 1])\n",
    "    dfs = clean_text_column(dfs, \"Примечание\", [1, 1, 0, 1, 1, 0])\n",
    "    \n",
    "    dfs = clean_text_column(dfs, \"Потребитель\", [1, 1, 0, 0, 0, 1])\n",
    "    dfs = clean_text_column(dfs, \"Отдел\", [1, 1, 0, 0, 0, 0])\n",
    "    dfs = clean_text_column(dfs, \"Ответственный\", [1, 1, 0, 0, 0, 0])\n",
    "    \n",
    "    return dfs\n",
    "    \n",
    "def data_preprocess_potreb(dfs):\n",
    "    dfs = clean_text_column(dfs, \"Потребитель\", [1, 1, 0, 0, 0, 1])\n",
    "    dfs = clean_text_column(dfs, \"Отдел\", [1, 1, 0, 0, 0, 0])\n",
    "    dfs = clean_text_column(dfs, \"Ответственный\", [1, 1, 0, 0, 0, 0])\n",
    "    dfs = clean_text_column(dfs, \"Материал\", [1, 1, 0, 1, 1, 1])\n",
    "    return dfs\n",
    "\n",
    "def data_preprocess_pp(dfs):\n",
    "    dfs = clean_text_column(dfs, \"Потребитель\", [1, 1, 0, 0, 0, 1])\n",
    "    dfs = clean_text_column(dfs, \"Отдел\", [1, 1, 0, 0, 0, 0])\n",
    "    dfs = clean_text_column(dfs, \"МП\", [1, 1, 0, 0, 0, 0])\n",
    "    dfs = clean_text_column(dfs, \"Материал, применение\", [1, 1, 0, 1, 1, 1])\n",
    "    dfs =clean_text_column(dfs, \"Товар\", [1, 1, 0, 0, 1, 0])\n",
    "    dfs =clean_text_column(dfs, \"Установка\", [1, 1, 0, 0, 0, 0])\n",
    "    dfs =clean_text_column(dfs, \"Производитель\", [1, 1, 0, 0, 0, 0])\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pivot = data_preprocess_pivot(dfs_pivot_original.copy())\n",
    "dfs_potrebnosti = data_preprocess_potreb(dfs_potrebnosti_orginal.copy())\n",
    "dfs_pivot = dfs_pivot.reset_index()\n",
    "dfs_potrebnosti = dfs_potrebnosti.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'pp': [],\n",
    "    'positive_potreb': [],\n",
    "    'negative_potreb': []\n",
    "}\n",
    "\n",
    "df_dataset = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "import random\n",
    "с = 0\n",
    "for i, pp_row in dfs_pivot.iterrows():\n",
    "    pp = ' '.join(np.array(pp_row[col4match_pivot_1].tolist()).astype(str))\n",
    "    positive_potreb = \" \".join(np.array(pp_row[col4match_pivot_2].tolist()).astype(str))\n",
    "    \n",
    "    matching_rows = dfs_potrebnosti[dfs_potrebnosti[\"Потребитель\"].apply(lambda x: fuzz.ratio(x, pp_row[\"Потребитель\"]) > 60)]\n",
    "    matching_rows = matching_rows[matching_rows[\"Отдел\"].apply(lambda x: fuzz.ratio(x, pp_row[\"Отдел\"]) > 60)]\n",
    "    matching_rows = matching_rows[matching_rows['Материал']!= pp_row[\"Материал потребитель\"]]\n",
    "    if len(matching_rows)!=0:\n",
    "        rand_indx = random.choice(matching_rows.index)\n",
    "        row = matching_rows.loc[rand_indx]\n",
    "    else:\n",
    "        rand_indx = random.choice(dfs_potrebnosti.index)\n",
    "        row = dfs_potrebnosti.loc[rand_indx]\n",
    "        с+=1\n",
    "    \n",
    "    negative_potreb = \" \".join(np.array(row[col4match_potreb_2].tolist()).astype(str))\n",
    "    new_index = len(df_dataset)\n",
    "    df_dataset.loc[new_index] = [pp, positive_potreb, negative_potreb]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = df_dataset.replace('nan', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## triplet loss ruBert model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wormsin/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class TripletBERT(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"DeepPavlov/rubert-base-cased\"):\n",
    "        super(TripletBERT, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Используем выход скрытого слоя на уровне [CLS] токена как эмбеддинг\n",
    "        return output.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "model = TripletBERT()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.bert.named_parameters():\n",
    "    if \"encoder.layer\" in name and int(name.split('.')[2]) < 8:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TripletTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Получаем строки\n",
    "        anchor = str(self.data.iloc[index, 0])\n",
    "        positive = str(self.data.iloc[index, 1])\n",
    "        negative = str(self.data.iloc[index, 2])\n",
    "\n",
    "        # Токенизация Anchor\n",
    "        anchor_inputs = self.tokenizer.encode_plus(\n",
    "            anchor,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Токенизация Positive\n",
    "        positive_inputs = self.tokenizer.encode_plus(\n",
    "            positive,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Токенизация Negative\n",
    "        negative_inputs = self.tokenizer.encode_plus(\n",
    "            negative,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(0),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(0),\n",
    "            'positive_input_ids': positive_inputs['input_ids'].squeeze(0),\n",
    "            'positive_attention_mask': positive_inputs['attention_mask'].squeeze(0),\n",
    "            'negative_input_ids': negative_inputs['input_ids'].squeeze(0),\n",
    "            'negative_attention_mask': negative_inputs['attention_mask'].squeeze(0),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\") \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripletTextDataset(df, tokenizer, max_length=64)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    # Косинусная близость\n",
    "    pos_distance = F.cosine_similarity(anchor, positive)\n",
    "    neg_distance = F.cosine_similarity(anchor, negative)\n",
    "\n",
    "    # Triplet Loss\n",
    "    loss = torch.mean(F.relu(neg_distance - pos_distance + margin))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.9062754958868027\n",
      "Epoch 2/200, Loss: 0.821341797709465\n",
      "Epoch 3/200, Loss: 0.7497813701629639\n",
      "Epoch 4/200, Loss: 0.6198317110538483\n",
      "Epoch 5/200, Loss: 0.5784109234809875\n",
      "Epoch 6/200, Loss: 0.5761526823043823\n",
      "Epoch 7/200, Loss: 0.4265815205872059\n",
      "Epoch 8/200, Loss: 0.5029339417815208\n",
      "Epoch 9/200, Loss: 0.3521309345960617\n",
      "Epoch 10/200, Loss: 0.35863395780324936\n",
      "Epoch 11/200, Loss: 0.3530420958995819\n",
      "Epoch 12/200, Loss: 0.267801932990551\n",
      "Epoch 13/200, Loss: 0.2833317443728447\n",
      "Epoch 14/200, Loss: 0.25822220370173454\n",
      "Epoch 15/200, Loss: 0.2816007360816002\n",
      "Epoch 16/200, Loss: 0.17239025980234146\n",
      "Epoch 17/200, Loss: 0.1647677905857563\n",
      "Epoch 18/200, Loss: 0.1715560145676136\n",
      "Epoch 19/200, Loss: 0.16969040036201477\n",
      "Epoch 20/200, Loss: 0.1473044566810131\n",
      "Epoch 21/200, Loss: 0.14785170927643776\n",
      "Epoch 22/200, Loss: 0.11724194884300232\n",
      "Epoch 23/200, Loss: 0.09736148081719875\n",
      "Epoch 24/200, Loss: 0.10676917806267738\n",
      "Epoch 25/200, Loss: 0.09608535282313824\n",
      "Epoch 26/200, Loss: 0.18496934603899717\n",
      "Epoch 27/200, Loss: 0.17159633710980415\n",
      "Epoch 28/200, Loss: 0.12039203941822052\n",
      "Epoch 29/200, Loss: 0.1507055815309286\n",
      "Epoch 30/200, Loss: 0.12450306303799152\n",
      "Epoch 31/200, Loss: 0.10656679142266512\n",
      "Epoch 32/200, Loss: 0.12077383510768414\n",
      "Epoch 33/200, Loss: 0.09825298190116882\n",
      "Epoch 34/200, Loss: 0.08327090553939342\n",
      "Epoch 35/200, Loss: 0.08552151918411255\n",
      "Epoch 36/200, Loss: 0.08215932920575142\n",
      "Epoch 37/200, Loss: 0.09308052808046341\n",
      "Epoch 38/200, Loss: 0.060002201702445745\n",
      "Epoch 39/200, Loss: 0.055075496435165405\n",
      "Epoch 40/200, Loss: 0.06651153974235058\n",
      "Epoch 41/200, Loss: 0.07691665552556515\n",
      "Epoch 42/200, Loss: 0.06018989998847246\n",
      "Epoch 43/200, Loss: 0.07573469541966915\n",
      "Epoch 44/200, Loss: 0.0518521461635828\n",
      "Epoch 45/200, Loss: 0.04520753864198923\n",
      "Epoch 46/200, Loss: 0.043083612341433764\n",
      "Epoch 47/200, Loss: 0.0544792739674449\n",
      "Epoch 48/200, Loss: 0.05061789136379957\n",
      "Epoch 49/200, Loss: 0.06354627851396799\n",
      "Epoch 50/200, Loss: 0.07977539673447609\n",
      "Epoch 51/200, Loss: 0.04950836766511202\n",
      "Epoch 52/200, Loss: 0.10158711485564709\n",
      "Epoch 53/200, Loss: 0.08920226898044348\n",
      "Epoch 54/200, Loss: 0.033539630472660065\n",
      "Epoch 55/200, Loss: 0.03967952961102128\n",
      "Epoch 56/200, Loss: 0.02770706545561552\n",
      "Epoch 57/200, Loss: 0.027537720277905464\n",
      "Epoch 58/200, Loss: 0.024569370318204165\n",
      "Epoch 59/200, Loss: 0.02782371360808611\n",
      "Epoch 60/200, Loss: 0.014503811951726675\n",
      "Epoch 61/200, Loss: 0.032828619703650475\n",
      "Epoch 62/200, Loss: 0.02661790419369936\n",
      "Epoch 63/200, Loss: 0.024839941877871752\n",
      "Epoch 64/200, Loss: 0.02780316350981593\n",
      "Epoch 65/200, Loss: 0.02577972924336791\n",
      "Epoch 66/200, Loss: 0.04363481421023607\n",
      "Epoch 67/200, Loss: 0.04202743619680405\n",
      "Epoch 68/200, Loss: 0.01673832628875971\n",
      "Epoch 69/200, Loss: 0.032996068708598614\n",
      "Epoch 70/200, Loss: 0.0335835344158113\n",
      "Epoch 71/200, Loss: 0.028299608267843723\n",
      "Epoch 72/200, Loss: 0.026711735408753157\n",
      "Epoch 73/200, Loss: 0.023671676870435476\n",
      "Epoch 74/200, Loss: 0.027174599934369326\n",
      "Epoch 75/200, Loss: 0.013695280067622662\n",
      "Epoch 76/200, Loss: 0.016396875027567148\n",
      "Epoch 77/200, Loss: 0.02616902207955718\n",
      "Epoch 78/200, Loss: 0.012980835977941751\n",
      "Epoch 79/200, Loss: 0.00786032434552908\n",
      "Epoch 80/200, Loss: 0.028075975365936756\n",
      "Epoch 81/200, Loss: 0.009423220995813608\n",
      "Epoch 82/200, Loss: 0.013773981481790543\n",
      "Epoch 83/200, Loss: 0.011398303788155317\n",
      "Epoch 84/200, Loss: 0.015111482236534357\n",
      "Epoch 85/200, Loss: 0.013914191629737616\n",
      "Epoch 86/200, Loss: 0.006498773582279682\n",
      "Epoch 87/200, Loss: 0.010892278980463743\n",
      "Epoch 88/200, Loss: 0.0074320826679468155\n",
      "Epoch 89/200, Loss: 0.006550702266395092\n",
      "Epoch 90/200, Loss: 0.00756880035623908\n",
      "Epoch 91/200, Loss: 0.009272996801882982\n",
      "Epoch 92/200, Loss: 0.012702585197985172\n",
      "Epoch 93/200, Loss: 0.010479447431862354\n",
      "Epoch 94/200, Loss: 0.010815402958542109\n",
      "Epoch 95/200, Loss: 0.03343206690624356\n",
      "Epoch 96/200, Loss: 0.009012036491185427\n",
      "Epoch 97/200, Loss: 0.010558615438640118\n",
      "Epoch 98/200, Loss: 0.003354147542268038\n",
      "Epoch 99/200, Loss: 0.010096488054841757\n",
      "Epoch 100/200, Loss: 0.0067075383849442005\n",
      "Epoch 101/200, Loss: 0.009181492496281862\n",
      "Epoch 102/200, Loss: 0.010057246778160334\n",
      "Epoch 103/200, Loss: 0.008183168713003397\n",
      "Epoch 104/200, Loss: 0.007970641367137432\n",
      "Epoch 105/200, Loss: 0.0054789320565760136\n",
      "Epoch 106/200, Loss: 0.00931072747334838\n",
      "Epoch 107/200, Loss: 0.0010480512864887714\n",
      "Epoch 108/200, Loss: 0.01000887993723154\n",
      "Epoch 109/200, Loss: 0.007586488034576178\n",
      "Epoch 110/200, Loss: 0.008169133681803942\n",
      "Epoch 111/200, Loss: 0.007331910077482462\n",
      "Epoch 112/200, Loss: 0.011583401821553707\n",
      "Epoch 113/200, Loss: 0.002921787556260824\n",
      "Epoch 114/200, Loss: 0.003474025521427393\n",
      "Epoch 115/200, Loss: 0.003122356254607439\n",
      "Epoch 116/200, Loss: 0.005598484538495541\n",
      "Epoch 117/200, Loss: 0.004392329137772322\n",
      "Epoch 118/200, Loss: 0.005448071286082268\n",
      "Epoch 119/200, Loss: 0.007700849324464798\n",
      "Epoch 120/200, Loss: 0.002377645578235388\n",
      "Epoch 121/200, Loss: 0.00454690121114254\n",
      "Epoch 122/200, Loss: 0.0034183557145297527\n",
      "Epoch 123/200, Loss: 0.003593286033719778\n",
      "Epoch 124/200, Loss: 0.0070410664193332195\n",
      "Epoch 125/200, Loss: 0.002199840731918812\n",
      "Epoch 126/200, Loss: 0.0058872937224805355\n",
      "Epoch 127/200, Loss: 0.0061117736622691154\n",
      "Epoch 128/200, Loss: 0.00831843912601471\n",
      "Epoch 129/200, Loss: 0.004149224143475294\n",
      "Epoch 130/200, Loss: 0.005206041969358921\n",
      "Epoch 131/200, Loss: 0.011841167230159044\n",
      "Epoch 132/200, Loss: 0.0004174797795712948\n",
      "Epoch 133/200, Loss: 0.0019810968078672886\n",
      "Epoch 134/200, Loss: 0.002923301886767149\n",
      "Epoch 135/200, Loss: 0.0033908961340785027\n",
      "Epoch 136/200, Loss: 0.0022054705768823624\n",
      "Epoch 137/200, Loss: 0.005114271771162748\n",
      "Epoch 138/200, Loss: 0.0011300500482320786\n",
      "Epoch 139/200, Loss: 0.0026730108074843884\n",
      "Epoch 140/200, Loss: 0.0030761463567614555\n",
      "Epoch 141/200, Loss: 0.0051167309284210205\n",
      "Epoch 142/200, Loss: 0.007008683402091265\n",
      "Epoch 143/200, Loss: 0.0012098397128283978\n",
      "Epoch 144/200, Loss: 0.003656554501503706\n",
      "Epoch 145/200, Loss: 0.010500306263566017\n",
      "Epoch 146/200, Loss: 0.008131257724016905\n",
      "Epoch 147/200, Loss: 0.0019453931599855423\n",
      "Epoch 148/200, Loss: 0.0012964974157512188\n",
      "Epoch 149/200, Loss: 0.007051969412714243\n",
      "Epoch 150/200, Loss: 0.0037905024364590645\n",
      "Epoch 151/200, Loss: 0.0013563139364123344\n",
      "Epoch 152/200, Loss: 0.0010086591355502605\n",
      "Epoch 153/200, Loss: 0.00033657392486929893\n",
      "Epoch 154/200, Loss: 0.001762431114912033\n",
      "Epoch 155/200, Loss: 0.0026456578634679317\n",
      "Epoch 156/200, Loss: 0.002447455655783415\n",
      "Epoch 157/200, Loss: 0.000919730868190527\n",
      "Epoch 158/200, Loss: 0.00031997356563806534\n",
      "Epoch 159/200, Loss: 0.002167114522308111\n",
      "Epoch 160/200, Loss: 0.0037935152649879456\n",
      "Epoch 161/200, Loss: 0.0008222353644669056\n",
      "Epoch 162/200, Loss: 0.0021302863024175167\n",
      "Epoch 163/200, Loss: 0.006071626674383879\n",
      "Epoch 164/200, Loss: 0.0010728915221989155\n",
      "Epoch 165/200, Loss: 0.00119455112144351\n",
      "Epoch 166/200, Loss: 0.0015254956670105457\n",
      "Epoch 167/200, Loss: 0.01417851634323597\n",
      "Epoch 168/200, Loss: 0.0008614598773419857\n",
      "Epoch 169/200, Loss: 0.0014246436767280102\n",
      "Epoch 170/200, Loss: 0.0032167905010282993\n",
      "Epoch 171/200, Loss: 8.827261626720428e-05\n",
      "Epoch 172/200, Loss: 0.003003331832587719\n",
      "Epoch 173/200, Loss: 0.0019883387722074986\n",
      "Epoch 174/200, Loss: 0.0002332325093448162\n",
      "Epoch 175/200, Loss: 0.003287249244749546\n",
      "Epoch 176/200, Loss: 0.004490375053137541\n",
      "Epoch 177/200, Loss: 0.008602025918662548\n",
      "Epoch 178/200, Loss: 0.0054573314264416695\n",
      "Epoch 179/200, Loss: 0.001649459358304739\n",
      "Epoch 180/200, Loss: 0.002823329996317625\n",
      "Epoch 181/200, Loss: 0.0010025370866060257\n",
      "Epoch 182/200, Loss: 0.0018014810048043728\n",
      "Epoch 183/200, Loss: 0.0019479990005493164\n",
      "Epoch 184/200, Loss: 0.00019371509552001953\n",
      "Epoch 185/200, Loss: 0.0018676398321986198\n",
      "Epoch 186/200, Loss: 0.0\n",
      "Epoch 187/200, Loss: 0.000256919302046299\n",
      "Epoch 188/200, Loss: 0.00042641861364245415\n",
      "Epoch 189/200, Loss: 0.00026227207854390144\n",
      "Epoch 190/200, Loss: 0.004560273140668869\n",
      "Epoch 191/200, Loss: 0.0\n",
      "Epoch 192/200, Loss: 0.000659882090985775\n",
      "Epoch 193/200, Loss: 0.0018940847367048264\n",
      "Epoch 194/200, Loss: 0.0\n",
      "Epoch 195/200, Loss: 0.0006835190579295158\n",
      "Epoch 196/200, Loss: 0.004526142030954361\n",
      "Epoch 197/200, Loss: 0.0\n",
      "Epoch 198/200, Loss: 0.0027118786238133907\n",
      "Epoch 199/200, Loss: 0.00284572783857584\n",
      "Epoch 200/200, Loss: 0.00019470276311039925\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        anchor_input_ids = batch['anchor_input_ids'].to(device)\n",
    "        anchor_attention_mask = batch['anchor_attention_mask'].to(device)\n",
    "        positive_input_ids = batch['positive_input_ids'].to(device)\n",
    "        positive_attention_mask = batch['positive_attention_mask'].to(device)\n",
    "        negative_input_ids = batch['negative_input_ids'].to(device)\n",
    "        negative_attention_mask = batch['negative_attention_mask'].to(device)\n",
    "\n",
    "        # Получаем эмбеддинги для Anchor, Positive и Negative\n",
    "        anchor_emb = model(anchor_input_ids, anchor_attention_mask)\n",
    "        positive_emb = model(positive_input_ids, positive_attention_mask)\n",
    "        negative_emb = model(negative_input_ids, negative_attention_mask)\n",
    "\n",
    "        # Вычисляем Triplet Loss\n",
    "        loss = triplet_loss(anchor_emb, positive_emb, negative_emb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.bert, 'bert_model/200_epochs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## предобучение модели на словаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs_pp = dfs_pp_original[col4match_pp]\n",
    "dfs_potrebnosti = dfs_potrebnosti_orginal[col4match_potreb_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pp['combined'] = dfs_pp[col4match_pp].apply(lambda row: ' [SEP] '.join(row.values.astype(str)), axis=1)\n",
    "dfs_potrebnosti['combined'] = dfs_potrebnosti[col4match_potreb_2].apply(lambda row: ' [SEP] '.join(row.values.astype(str)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pp['combined'] = dfs_pp['combined'].replace('nan', '', regex=True)\n",
    "dfs_potrebnosti['combined'] = dfs_potrebnosti['combined'].replace('nan', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26127"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs_potrebnosti['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'combined':[]\n",
    "\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['combined'] = pd.concat((dfs_pp['combined'], dfs_potrebnosti['combined']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>РН-Сызранский НПЗ, АО [SEP] КиА НП [SEP] Жилен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ЛУКОЙЛ-Пермнефтеоргсинтез [SEP] КиА НП [SEP] Ш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ЛУКОЙЛ-Пермнефтеоргсинтез [SEP] КиА НП [SEP] Ш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ЛУКОЙЛ-Пермнефтеоргсинтез [SEP] КиА НП [SEP] Ш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ЛУКОЙЛ-Пермнефтеоргсинтез [SEP] КиА НП [SEP] Ш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26122</th>\n",
       "      <td>Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26123</th>\n",
       "      <td>Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26124</th>\n",
       "      <td>Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26125</th>\n",
       "      <td>Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26126</th>\n",
       "      <td>Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26407 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                combined\n",
       "0      РН-Сызранский НПЗ, АО [SEP] КиА НП [SEP] Жилен...\n",
       "1      ЛУКОЙЛ-Пермнефтеоргсинтез [SEP] КиА НП [SEP] Ш...\n",
       "2      ЛУКОЙЛ-Пермнефтеоргсинтез [SEP] КиА НП [SEP] Ш...\n",
       "3      ЛУКОЙЛ-Пермнефтеоргсинтез [SEP] КиА НП [SEP] Ш...\n",
       "4      ЛУКОЙЛ-Пермнефтеоргсинтез [SEP] КиА НП [SEP] Ш...\n",
       "...                                                  ...\n",
       "26122  Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...\n",
       "26123  Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...\n",
       "26124  Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...\n",
       "26125  Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...\n",
       "26126  Башнефть-Новойл [SEP] КиА НП [SEP] Жиленко [SE...\n",
       "\n",
       "[26407 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем объединенные строки в txt-файл\n",
    "df['combined'].to_csv('data_for_finetuning.txt', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wormsin/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wormsin/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 10%|█         | 500/4959 [03:37<31:32,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3704, 'grad_norm': 8.495681762695312, 'learning_rate': 4.495866102036701e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1000/4959 [07:12<28:39,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9021, 'grad_norm': 9.423467636108398, 'learning_rate': 3.991732204073402e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1500/4959 [10:47<24:30,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7483, 'grad_norm': 12.327615737915039, 'learning_rate': 3.487598306110103e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2000/4959 [14:22<21:32,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6476, 'grad_norm': 7.818604469299316, 'learning_rate': 2.983464408146804e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2500/4959 [17:57<17:30,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5775, 'grad_norm': 7.46843957901001, 'learning_rate': 2.479330510183505e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3000/4959 [21:32<13:59,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5609, 'grad_norm': 4.899144172668457, 'learning_rate': 1.975196612220206e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3500/4959 [25:07<10:20,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4971, 'grad_norm': 5.839532852172852, 'learning_rate': 1.4710627142569067e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 4000/4959 [28:42<06:53,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4594, 'grad_norm': 10.365859031677246, 'learning_rate': 9.669288162936076e-06, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 4500/4959 [32:17<03:16,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4391, 'grad_norm': 8.54740047454834, 'learning_rate': 4.627949183303086e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4959/4959 [35:34<00:00,  2.42it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 47\u001b[0m\n\u001b[1;32m     39\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     40\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     41\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     42\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     43\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,  \u001b[38;5;66;03m# Для автоматической маскировки токенов\u001b[39;00m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Дообучаем модель на вашем словаре\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Сохраняем дообученную модель\u001b[39;00m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert_finetuned\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/trainer.py:2356\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/trainer.py:2807\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2804\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2807\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2808\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/trainer.py:2886\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2884\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   2885\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 2886\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/trainer.py:3454\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/trainer.py:3525\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3523\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   3524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3525\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3526\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   3527\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/modeling_utils.py:2793\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2788\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[1;32m   2791\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[0;32m-> 2793\u001b[0m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2795\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/safetensors/torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/safetensors/torch.py:500\u001b[0m, in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    497\u001b[0m     k: {\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m--> 500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    501\u001b[0m     }\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    503\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/file_clean/lib/python3.12/site-packages/safetensors/torch.py:414\u001b[0m, in \u001b[0;36m_tobytes\u001b[0;34m(tensor, name)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to save a sparse tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` which this library does not support.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can make it a dense tensor before saving with `.to_dense()` but be aware this might\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make a much larger file than needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_contiguous():\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` which is not allowed. It either means you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms recommended to save\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pack it before saving.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     )\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Загружаем предобученную модель и токенайзер\n",
    "model = BertForMaskedLM.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "# Подготавливаем датасет из вашего словаря\n",
    "dataset = load_dataset('text', data_files={'train': 'data_for_finetuning.txt'})\n",
    "#dataset = dataset['train'].map(lambda examples: tokenizer(examples['text'], truncation=True, padding='max_length', max_length=64), batched=True)\n",
    "\n",
    "# Шаг 3: Токенизация данных\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Шаг 4: Использование DataCollator для автоматической маскировки токенов\n",
    "# DataCollatorForLanguageModeling автоматически замаскирует часть токенов\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=True, \n",
    "    mlm_probability=0.15  # 15% токенов будут замаскированы\n",
    ")\n",
    "\n",
    "# Шаг 5: Аргументы для тренировки\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_finetuned\",  # Директория для сохранения модели\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,  # Оставляем только потери (loss) для упрощения вывода\n",
    ")\n",
    "\n",
    "# Шаг 6: Создание Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    data_collator=data_collator,  # Для автоматической маскировки токенов\n",
    ")\n",
    "\n",
    "# Дообучаем модель на вашем словаре\n",
    "trainer.train()\n",
    "\n",
    "# Сохраняем дообученную модель\n",
    "model.save_pretrained('bert_finetuned')\n",
    "tokenizer.save_pretrained('bert_finetuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.bert, 'bert_finetuned/3_epochs_bert.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_finetuned/tokenizer/tokenizer_config.json',\n",
       " 'bert_finetuned/tokenizer/special_tokens_map.json',\n",
       " 'bert_finetuned/tokenizer/vocab.txt',\n",
       " 'bert_finetuned/tokenizer/added_tokens.json',\n",
       " 'bert_finetuned/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('bert_finetuned/tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pivot_table = \"../data/pivot_table.xlsx\"\n",
    "dfs = pd.read_excel(pivot_table, sheet_name=None)\n",
    "dfs_pp_original = dfs[\"ПП\"]\n",
    "dfs_potrebnosti_orginal = dfs[\"Потребности\"]\n",
    "dfs_potrebnosti = data_preprocess_potreb(dfs_potrebnosti_orginal.copy())\n",
    "dfs_potrebnosti = dfs_potrebnosti.reset_index()\n",
    "dfs_pp = data_preprocess_pp(dfs_pp_original.copy())\n",
    "dfs_pp = dfs_pp.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wormsin/miniconda3/envs/file_clean/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_6540/2228023973.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('bert_finetuned/3_epochs_bert.pth').to('cpu')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "model = torch.load('bert_finetuned/3_epochs_bert.pth').to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'string':[]\n",
    "}\n",
    "\n",
    "df_vec1= pd.DataFrame(data)\n",
    "df_vec2= pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vec_data(col4match, max_length, dfs, df):\n",
    "    for indx, row in dfs.iterrows():\n",
    "        anchor = ' '.join(np.array(row[col4match].tolist()).astype(str))\n",
    "        inputs = tokenizer.encode_plus(\n",
    "                    anchor,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        df.loc[indx] = [outputs.last_hidden_state[:, 0, :].numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "col4match = [\"Потребитель\", \"Отдел\" ,\"МП\", \"Материал, применение\", 'Установка', 'Товар', 'Производитель']\n",
    "create_vec_data(col4match, max_length=64, dfs=dfs_pp, df=df_vec1)\n",
    "#create_vec_data(col4match_potreb_2, max_length=64, dfs=dfs_potrebnosti, df=df_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vec_data(col4match_potreb_2, max_length=64, dfs=dfs_potrebnosti, df=df_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = np.vstack(df_vec1['string'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = np.vstack(df_vec2['string'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Индексы ближайших соседей: [[20945 14863 14409 23565 13909]\n",
      " [14483 17272 25515  1029 21123]\n",
      " [12691 10591 18329 10665 19571]\n",
      " ...\n",
      " [13578 24346 13118 10604 18149]\n",
      " [   50 23045 23443  2247 21088]\n",
      " [22095 18741 25515 24494 18485]]\n",
      "Расстояния до ближайших соседей: [[0.91479737 0.9108299  0.9082465  0.90682155 0.9062322 ]\n",
      " [0.94147086 0.9353296  0.9351766  0.9349063  0.93440205]\n",
      " [0.9170652  0.9094411  0.9078841  0.9075177  0.9072263 ]\n",
      " ...\n",
      " [0.7421916  0.72082764 0.71973836 0.71924835 0.71888053]\n",
      " [0.9601269  0.95820415 0.9561344  0.95613277 0.95416963]\n",
      " [0.94834495 0.94444263 0.9432762  0.9429506  0.9414692 ]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Примерные данные (вместо этого используйте ваши векторы)\n",
    "# 300 векторов с классами (например, 128-мерные)\n",
    "table_a_vectors = vec1\n",
    "\n",
    "# 30,000 векторов без классов\n",
    "table_b_vectors = vec2\n",
    "\n",
    "index = faiss.IndexFlatIP(768)\n",
    "\n",
    "# Добавляем нормализованные вектора\n",
    "faiss.normalize_L2(table_a_vectors)\n",
    "faiss.normalize_L2(table_b_vectors)\n",
    "index.add(table_b_vectors)\n",
    "# Создание индекса\n",
    "#index = faiss.IndexFlatL2(768)  # Индекс с L2 расстоянием\n",
    "#index.add(table_b_vectors)  # Добавление векторов из первой таблицы\n",
    "\n",
    "# Поиск ближайших соседей для векторов из второй таблицы\n",
    "k = 5  # Количество ближайших соседей\n",
    "distances, indices = index.search(table_a_vectors, k)\n",
    "\n",
    "# Индексы ближайших соседей и их расстояния\n",
    "print(\"Индексы ближайших соседей:\", indices)\n",
    "print(\"Расстояния до ближайших соседей:\", distances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indices)):\n",
    "    indice = indices[i][indices[i]<10000]\n",
    "    for indx in indice:\n",
    "        print('Строка ПП: \\n')\n",
    "        print(dfs_pp.loc[i][col4match])\n",
    "        print('Строки ПОтребности: \\n')\n",
    "        print(dfs_potrebnosti.loc[indx][col4match_potreb_2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтрация результатов\n",
    "matches = []\n",
    "for i in range(len(indices)):\n",
    "    matches.append((i, indices[i][indices[i]>0.5]))  # Сохраните индекс строки из table_b и соответствующий индекс из table_a\n",
    "\n",
    "# Сохранение результатов\n",
    "matches_df = pd.DataFrame(matches, columns=['table_a_index', 'table_b_index'])\n",
    "matches_df.to_csv('matches_finetuned_cos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = \"../data/pivot_table.xlsx\"\n",
    "dfs = pd.read_excel(pivot_table, sheet_name=None)\n",
    "dfs_pivot_original = dfs[\"Сводная ПП+Потребности\"]\n",
    "dfs_potrebnosti_orginal = dfs[\"Потребности\"]\n",
    "dfs_pp_original = dfs[\"ПП\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_potrebnosti = data_preprocess_potreb(dfs_potrebnosti_orginal.copy())\n",
    "dfs_potrebnosti = dfs_potrebnosti.reset_index()\n",
    "dfs_pp = data_preprocess_pp(dfs_pp_original.copy())\n",
    "dfs_pp = dfs_pp.reset_index()\n",
    "dfs_pivot = data_preprocess_pivot(dfs_pivot_original.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv('matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_a_index</th>\n",
       "      <th>table_b_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[ 354 9719  344]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[  60 9745]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>266</td>\n",
       "      <td>[2475   32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>267</td>\n",
       "      <td>[2475]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>268</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>269</td>\n",
       "      <td>[9713]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>[2475]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     table_a_index     table_b_index\n",
       "0                0                []\n",
       "1                1                []\n",
       "2                2  [ 354 9719  344]\n",
       "3                3                []\n",
       "4                4       [  60 9745]\n",
       "..             ...               ...\n",
       "266            266       [2475   32]\n",
       "267            267            [2475]\n",
       "268            268                []\n",
       "269            269            [9713]\n",
       "270            270            [2475]\n",
       "\n",
       "[271 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "manual  = []\n",
    "res = 0\n",
    "for i, row in dfs_pivot.iterrows():\n",
    "    pp_row = dfs_pp[(dfs_pp[\"Потребитель\"] == row[\"Потребитель\"]) &\n",
    "                    (dfs_pp[\"Отдел\"] == row[\"Отдел\"]) &\n",
    "                    (dfs_pp[\"МП\"] == row[\"Ответственный\"]) &\n",
    "                    (dfs_pp[\"Материал, применение\"] == row[\"Материал ПП\"]) &\n",
    "                    (dfs_pp[\"Установка\"] == row[\"Установка\"]) &\n",
    "                    (dfs_pp[\"Товар\"] == row[\"Товар\"])].index\n",
    "    if len(pp_row) ==1:\n",
    "        indx = pp_row[0]\n",
    "        matches_indices = list(map(int, matches.loc[indx][\"table_b_index\"].strip('[]').split()))\n",
    "        print(f\"New: {row[\"Материал потребитель\"]}\")\n",
    "        for k in matches_indices:\n",
    "            print(dfs_potrebnosti.loc[k]['Материал'])\n",
    "            if dfs_potrebnosti.loc[k]['Материал'] == row[\"Материал потребитель\"]:\n",
    "                res+=1\n",
    "    else:\n",
    "        manual+=pp_row.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "file_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
